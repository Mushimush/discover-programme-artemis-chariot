/**
 * ESP32-CAM with TensorFlow Lite Micro - Object Detection Example
 *
 * This is Phase 2: ML inference with TFLite Micro
 * This file shows how to add object detection to your ESP32-CAM
 *
 * NOTE: This is an EXAMPLE template. To use it:
 * 1. First get basic camera working (main.cpp)
 * 2. Choose your ML approach (Edge Impulse or TFLite Micro)
 * 3. Add required libraries to platformio.ini
 * 4. Add your model file
 * 5. Adapt this code for your specific model
 * 6. Rename to main.cpp (backup original first!)
 *
 * This example shows the general structure - you'll need to customize
 * for your specific model and framework.
 */

#include <Arduino.h>
#include "esp_camera.h"
#include "esp_log.h"

// ============================================================================
// CONFIGURATION - Update these for your setup
// ============================================================================

// Model Configuration
// Update these based on your actual model
#define MODEL_INPUT_WIDTH  96    // Model expects 96x96 input (adjust for your model)
#define MODEL_INPUT_HEIGHT 96
#define MODEL_INPUT_CHANNELS 1   // 1 for grayscale, 3 for RGB

// Detection Configuration
#define CONFIDENCE_THRESHOLD 0.6  // Only report detections above 60% confidence
#define INFERENCE_INTERVAL_MS 500 // Run inference every 500ms (2 FPS)

// ============================================================================
// ESP32-CAM Pin Definitions (AI-Thinker board)
// ============================================================================
#define PWDN_GPIO_NUM     32
#define RESET_GPIO_NUM    -1
#define XCLK_GPIO_NUM      0
#define SIOD_GPIO_NUM     26
#define SIOC_GPIO_NUM     27
#define Y9_GPIO_NUM       35
#define Y8_GPIO_NUM       34
#define Y7_GPIO_NUM       39
#define Y6_GPIO_NUM       36
#define Y5_GPIO_NUM       21
#define Y4_GPIO_NUM       19
#define Y3_GPIO_NUM       18
#define Y2_GPIO_NUM        5
#define VSYNC_GPIO_NUM    25
#define HREF_GPIO_NUM     23
#define PCLK_GPIO_NUM     22
#define LED_GPIO_NUM       4

// ============================================================================
// OPTION A: Using Edge Impulse
// ============================================================================
// Uncomment this section if using Edge Impulse

/*
// Include your Edge Impulse library
// (download from Edge Impulse deployment and add to lib/)
#include <your-project-name_inferencing.h>

// Edge Impulse provides:
// - ei_classifier_init()
// - ei_classifier_run()
// - EI_CLASSIFIER_INPUT_WIDTH, EI_CLASSIFIER_INPUT_HEIGHT
// - Feature array for input

// Edge Impulse inference function
void runEdgeImpulseInference(camera_fb_t *fb) {
    // Prepare signal (image data)
    signal_t signal;
    signal.total_length = EI_CLASSIFIER_INPUT_WIDTH * EI_CLASSIFIER_INPUT_HEIGHT;
    signal.get_data = &get_signal_data;  // Your preprocessing function

    // Run inference
    ei_impulse_result_t result;
    EI_IMPULSE_ERROR res = run_classifier(&signal, &result, false);

    if (res != 0) {
        Serial.printf("ERR: Failed to run classifier (%d)\n", res);
        return;
    }

    // Print timing info
    Serial.printf("Inference: %d ms, DSP: %d ms, Classification: %d ms\n",
                  result.timing.dsp + result.timing.classification,
                  result.timing.dsp,
                  result.timing.classification);

    // Print predictions
    Serial.println("Predictions:");
    for (size_t i = 0; i < EI_CLASSIFIER_LABEL_COUNT; i++) {
        if (result.classification[i].value > CONFIDENCE_THRESHOLD) {
            Serial.printf("  %s: %.2f\n",
                         result.classification[i].label,
                         result.classification[i].value);
        }
    }

    // Object detection results (if using FOMO)
    #if EI_CLASSIFIER_OBJECT_DETECTION == 1
    Serial.printf("Detected %d objects:\n", result.bounding_boxes_count);
    for (size_t i = 0; i < result.bounding_boxes_count; i++) {
        ei_impulse_result_bounding_box_t bb = result.bounding_boxes[i];
        if (bb.value > CONFIDENCE_THRESHOLD) {
            Serial.printf("  %s (%.2f) at x:%d y:%d w:%d h:%d\n",
                         bb.label,
                         bb.value,
                         bb.x, bb.y,
                         bb.width, bb.height);
        }
    }
    #endif
}
*/

// ============================================================================
// OPTION B: Using TensorFlow Lite Micro
// ============================================================================
// Uncomment this section if using TFLite Micro directly

/*
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/micro/micro_mutable_op_resolver.h"
#include "tensorflow/lite/micro/system_setup.h"
#include "tensorflow/lite/schema/schema_generated.h"

// Include your model (convert .tflite to C array first)
#include "model_data.h"  // Contains: const unsigned char model_data[]

// TFLite globals
namespace {
    const tflite::Model* model = nullptr;
    tflite::MicroInterpreter* interpreter = nullptr;
    TfLiteTensor* input = nullptr;
    TfLiteTensor* output = nullptr;

    // Tensor arena size - adjust based on your model
    // Start with 100KB and increase if you get allocation errors
    constexpr int kTensorArenaSize = 100 * 1024;
    uint8_t tensor_arena[kTensorArenaSize];
}

// Initialize TFLite
bool initTFLite() {
    // Load model
    model = tflite::GetModel(model_data);
    if (model->version() != TFLITE_SCHEMA_VERSION) {
        Serial.printf("Model schema version %d not supported. Supported version is %d\n",
                     model->version(), TFLITE_SCHEMA_VERSION);
        return false;
    }

    // Define operations used in your model
    // Add only the ops your model uses to save memory
    static tflite::MicroMutableOpResolver<10> micro_op_resolver;
    micro_op_resolver.AddConv2D();
    micro_op_resolver.AddDepthwiseConv2D();
    micro_op_resolver.AddFullyConnected();
    micro_op_resolver.AddMaxPool2D();
    micro_op_resolver.AddSoftmax();
    micro_op_resolver.AddReshape();
    micro_op_resolver.AddQuantize();
    micro_op_resolver.AddDequantize();
    // Add more ops as needed for your model

    // Build interpreter
    static tflite::MicroInterpreter static_interpreter(
        model, micro_op_resolver, tensor_arena, kTensorArenaSize);
    interpreter = &static_interpreter;

    // Allocate tensors
    TfLiteStatus allocate_status = interpreter->AllocateTensors();
    if (allocate_status != kTfLiteOk) {
        Serial.println("AllocateTensors() failed");
        return false;
    }

    // Get input/output tensors
    input = interpreter->input(0);
    output = interpreter->output(0);

    // Print model info
    Serial.println("Model loaded successfully!");
    Serial.printf("Input shape: [%d, %d, %d, %d]\n",
                 input->dims->data[0], input->dims->data[1],
                 input->dims->data[2], input->dims->data[3]);
    Serial.printf("Input type: %d (0=float32, 1=int8)\n", input->type);
    Serial.printf("Tensor arena used: %d / %d bytes\n",
                 interpreter->arena_used_bytes(), kTensorArenaSize);

    return true;
}

// Preprocess image for model input
void preprocessImage(camera_fb_t *fb, int8_t* input_buffer) {
    // This is a simplified example for 96x96 grayscale INT8 quantized model
    // You'll need to adjust for your specific model requirements

    // For RGB888 or JPEG, first decode and convert
    // For grayscale, resize and quantize

    // Example: Simple downsampling for grayscale
    int stride_w = fb->width / MODEL_INPUT_WIDTH;
    int stride_h = fb->height / MODEL_INPUT_HEIGHT;

    for (int y = 0; y < MODEL_INPUT_HEIGHT; y++) {
        for (int x = 0; x < MODEL_INPUT_WIDTH; x++) {
            // Sample pixel from frame buffer
            int src_x = x * stride_w;
            int src_y = y * stride_h;
            int src_idx = src_y * fb->width + src_x;

            // Get pixel value (assumes grayscale or luminance)
            uint8_t pixel = fb->buf[src_idx];

            // Quantize to INT8 (-128 to 127)
            // Adjust scale and zero_point based on your model
            input_buffer[y * MODEL_INPUT_WIDTH + x] = (int8_t)(pixel - 128);
        }
    }
}

// Run TFLite inference
void runTFLiteInference(camera_fb_t *fb) {
    unsigned long start_time = millis();

    // Preprocess image
    int8_t* input_buffer = input->data.int8;  // or .f if float model
    preprocessImage(fb, input_buffer);

    // Run inference
    TfLiteStatus invoke_status = interpreter->Invoke();
    if (invoke_status != kTfLiteOk) {
        Serial.println("Invoke failed!");
        return;
    }

    unsigned long inference_time = millis() - start_time;

    // Get results
    int8_t* output_buffer = output->data.int8;  // or .f if float

    // Parse outputs based on your model type:

    // For classification model (example):
    int output_size = output->dims->data[1];
    int max_idx = 0;
    int8_t max_val = output_buffer[0];
    for (int i = 1; i < output_size; i++) {
        if (output_buffer[i] > max_val) {
            max_val = output_buffer[i];
            max_idx = i;
        }
    }

    // Dequantize score (adjust based on your model's quantization params)
    float score = (max_val - output->params.zero_point) * output->params.scale;

    Serial.printf("Inference: %lu ms | Detected class: %d | Score: %.2f\n",
                 inference_time, max_idx, score);

    // For object detection model, you'll need to parse bounding boxes
    // This varies by model (SSD, YOLO, FOMO, etc.)
}
*/

// ============================================================================
// OPTION C: Using EloquentTinyML (Simplified wrapper)
// ============================================================================
// Uncomment if using EloquentTinyML library

/*
#include <eloquent_esp32cam.h>
#include <eloquent_esp32cam/edgeimpulse/fomo.h>

using eloq::camera;
using eloq::ei::fomo;

void runEloquentInference() {
    if (!camera.capture().isOk()) {
        Serial.println(camera.exception.toString());
        return;
    }

    // Run inference (EloquentTinyML handles preprocessing)
    if (!fomo.detectObjects().isOk()) {
        Serial.println(fomo.exception.toString());
        return;
    }

    // Print results
    Serial.printf("Found %d objects in %dms:\n",
                 fomo.count(),
                 fomo.getInferenceTime());

    fomo.forEach([](int i, bbox_t bbox) {
        Serial.printf("  Object %d: %s (%.2f) at (%d,%d) %dx%d\n",
                     i,
                     bbox.label,
                     bbox.proba,
                     bbox.x, bbox.y,
                     bbox.width, bbox.height);
    });
}
*/

// ============================================================================
// Camera Initialization
// ============================================================================

bool initCamera() {
    camera_config_t config;

    config.ledc_channel = LEDC_CHANNEL_0;
    config.ledc_timer = LEDC_TIMER_0;
    config.pin_d0 = Y2_GPIO_NUM;
    config.pin_d1 = Y3_GPIO_NUM;
    config.pin_d2 = Y4_GPIO_NUM;
    config.pin_d3 = Y5_GPIO_NUM;
    config.pin_d4 = Y6_GPIO_NUM;
    config.pin_d5 = Y7_GPIO_NUM;
    config.pin_d6 = Y8_GPIO_NUM;
    config.pin_d7 = Y9_GPIO_NUM;
    config.pin_xclk = XCLK_GPIO_NUM;
    config.pin_pclk = PCLK_GPIO_NUM;
    config.pin_vsync = VSYNC_GPIO_NUM;
    config.pin_href = HREF_GPIO_NUM;
    config.pin_sscb_sda = SIOD_GPIO_NUM;
    config.pin_sscb_scl = SIOC_GPIO_NUM;
    config.pin_pwdn = PWDN_GPIO_NUM;
    config.pin_reset = RESET_GPIO_NUM;
    config.xclk_freq_hz = 20000000;

    // For ML, use RGB565 or GRAYSCALE instead of JPEG
    // This avoids JPEG decoding overhead
    #if MODEL_INPUT_CHANNELS == 1
        config.pixel_format = PIXFORMAT_GRAYSCALE;
    #else
        config.pixel_format = PIXFORMAT_RGB565;
    #endif

    // Match model input size if possible, or downsample later
    // 96x96 is a good size for ESP32-CAM ML models
    config.frame_size = FRAMESIZE_96X96;

    config.jpeg_quality = 12;
    config.fb_count = 1;
    config.grab_mode = CAMERA_GRAB_WHEN_EMPTY;

    // Use PSRAM if available
    if (psramFound()) {
        config.fb_count = 2;
        config.grab_mode = CAMERA_GRAB_LATEST;
    }

    esp_err_t err = esp_camera_init(&config);
    if (err != ESP_OK) {
        Serial.printf("Camera init failed: 0x%x\n", err);
        return false;
    }

    Serial.println("Camera initialized!");
    return true;
}

// ============================================================================
// Main Setup and Loop
// ============================================================================

void setup() {
    Serial.begin(115200);
    delay(2000);

    Serial.println("\n========================================");
    Serial.println("ESP32-CAM TensorFlow Lite Object Detection");
    Serial.println("========================================\n");

    // Initialize camera
    if (!initCamera()) {
        Serial.println("Camera init failed!");
        while(1) delay(1000);
    }

    // Initialize ML framework (uncomment based on your choice)

    // For TFLite Micro:
    // if (!initTFLite()) {
    //     Serial.println("TFLite init failed!");
    //     while(1) delay(1000);
    // }

    // For Edge Impulse:
    // if (ei_classifier_init() != EI_IMPULSE_OK) {
    //     Serial.println("Edge Impulse init failed!");
    //     while(1) delay(1000);
    // }

    // For EloquentTinyML:
    // camera.pinout.aithinker();
    // camera.brownout.disable();
    // camera.resolution.face();  // or .sqcif96x96()
    // if (!camera.begin().isOk()) {
    //     Serial.println(camera.exception.toString());
    //     while(1) delay(1000);
    // }

    Serial.println("\n========================================");
    Serial.println("Setup complete! Starting inference...");
    Serial.println("========================================\n");
}

void loop() {
    static unsigned long last_inference = 0;

    // Rate limit inference
    if (millis() - last_inference < INFERENCE_INTERVAL_MS) {
        delay(10);
        return;
    }
    last_inference = millis();

    // Capture frame
    camera_fb_t *fb = esp_camera_fb_get();
    if (!fb) {
        Serial.println("Camera capture failed");
        delay(100);
        return;
    }

    Serial.printf("\n--- Frame %lu ---\n", millis());
    Serial.printf("Size: %d bytes, %dx%d\n", fb->len, fb->width, fb->height);

    // Run inference (uncomment based on your choice)

    // Option A: Edge Impulse
    // runEdgeImpulseInference(fb);

    // Option B: TensorFlow Lite Micro
    // runTFLiteInference(fb);

    // Option C: EloquentTinyML
    // runEloquentInference();  // Uses internal capture

    // Return frame buffer
    esp_camera_fb_return(fb);

    // Print memory stats
    Serial.printf("Free heap: %d bytes\n", ESP.getFreeHeap());
    Serial.println("---");
}

/**
 * TO USE THIS EXAMPLE:
 *
 * 1. Choose your ML framework (Edge Impulse, TFLite Micro, or EloquentTinyML)
 *
 * 2. Update platformio.ini with required libraries:
 *    - Edge Impulse: Add your downloaded library
 *    - TFLite Micro: Add tensorflow/tflite-micro-arduino-examples
 *    - EloquentTinyML: Add eloquentarduino libraries
 *
 * 3. Add your model file:
 *    - Edge Impulse: Comes with downloaded library
 *    - TFLite Micro: Create model_data.h from .tflite file
 *    - EloquentTinyML: Follows their model format
 *
 * 4. Uncomment the relevant sections above
 *
 * 5. Adjust preprocessing for your model:
 *    - Input dimensions (width x height)
 *    - Color format (grayscale vs RGB)
 *    - Quantization parameters
 *
 * 6. Adjust output parsing for your model type:
 *    - Classification: Get top class and score
 *    - Object detection: Parse bounding boxes
 *
 * 7. Test and optimize:
 *    - Adjust INFERENCE_INTERVAL_MS for desired FPS
 *    - Tune CONFIDENCE_THRESHOLD
 *    - Monitor memory usage
 *
 * See ESP32_OBJECT_RECOGNITION_SETUP.md for detailed instructions!
 */
